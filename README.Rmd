---
output: github_document
editor_options: 
  chunk_output_type: console
---

<!-- README.md is generated from README.Rmd. Please edit that file -->

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.path = "man/figures/README-",
  out.width = "100%",
  warning = FALSE
)
```

# celavi

<!-- badges: start -->
[![R-CMD-check](https://github.com/jbkunst/celavi/actions/workflows/R-CMD-check.yaml/badge.svg)](https://github.com/jbkunst/celavi/actions/workflows/R-CMD-check.yaml)
<!-- badges: end -->

The goal of celavi is to join the main features of two functions  
that I use _really_ often `vip::vi_permute` and `DALEX::model_parts`. Both functions
do the _same_ task of calculate _drop out loss via permutation_, 
but they have different features and approach.

In the case of `vip::vi_permute` is more direct to use (imho), have an implementation for
parallel processing, can be used with a `sample_frac` parameter. Otherwise, in the case of `DALEX::model_parts` I like the user can give custom `metric`s as a loss functions,
the _base line_ and _full model_ references values, and the plots. 

To that features I added some features to my _personal_ taste.

- Add progress bars to the sequential and parallel process using `progress::progress_bar` and `progressr::progress`
- Give the possibility of to the user to access to the _raw_ data.
- Verbose information using `cli::cli_alert_info`.

## References

The `vip` package from [koalaverse](https://github.com/koalaverse), and the 
`DALEX` package from [MIÂ²](https://www.mi2.ai/). In particular these links are awesome: https://koalaverse.github.io/vip/articles/vip.html
and https://ema.drwhy.ai/featureImportance.html#featureImportanceR.

Please, visit the links and used that awesome tools!

## Installation

You can install the development version of celavi from [GitHub](https://github.com/) with:

``` r
# install.packages("devtools")
devtools::install_github("jbkunst/celavi")
```

## Example I: Variable Importance

```{r example}
library(celavi)

lm_model <- lm(mpg ~ ., data = mtcars)

set.seed(123)

vi <- celavi::variable_importance(lm_model, data = mtcars, iterations = 10)

dplyr::glimpse(vi)

nrow(vi)
# nrow(vi) = (ncol(mtcars) - 1 + 2) * iterations

plot(vi)
```

And compare with other model.

```{r}
rf <- randomForest::randomForest(mpg ~ ., data = mtcars)

vi_rf <- celavi::variable_importance(rf, data = mtcars, iterations = 10)

plot(vi, vi_rf)
```

From the previous chart we can tell the random Forest have small (better) RMSE
and is less affected in terms of predictability by removing variables, wt variable
for example.

## Example II: Feature Selection

```{r}
set.seed(123)

data(credit_data, package = "modeldata")

credit_data <- credit_data[complete.cases(credit_data),]
credit_data$Status <- as.numeric(credit_data$Status) - 1

# convert factor to dummies (to compare results with glmnet)
credit_data <- as.data.frame(model.matrix(~ . - 1, data = credit_data))

trn_tst <- sample(
  c(TRUE, FALSE),
  size = nrow(credit_data),
  replace = TRUE, 
  prob = c(.7, .3)
  )

credit_data_trn <- credit_data[ trn_tst,]
credit_data_tst <- credit_data[!trn_tst,]

fs <- feature_selection(
  glm,
  credit_data_trn,
  response = "Status",
  stat = min,
  iterations = 10,
  sample_frac = 1, 
  predict_function = predict.glm,
  # function accepts specific argument for the fit function
  family  = binomial
)

fs

plot(fs)
```

We have a simpler model without loss significance predictive performance.

Nopw we can compare with some other feature selection techniques.

```{r, results='hide'}
mod_fs <- attr(fs, "final_fit")

mod_full <- glm(Status ~ ., data = credit_data_trn, family = binomial)

mod_step <- step(mod_full, trace = FALSE) 

# wrapper around glmnet::cv.glmnet()
mod_lasso <- risk3r::featsel_glmnet(mod_full, plot = FALSE)
```

```{r}
models <- list(
  "fs by vip" = mod_fs,
  "stepwise"  = mod_step,
  "lasso"     = mod_lasso
)

purrr::map_df(
  models,
  risk3r::model_metrics,
  newdata = credit_data_tst, 
  .id = "method"
)
```

Not the best model in terms of metrics. But if we see the number of coefficients:

```{r}
purrr::map_df(
  models,
  ~ tibble::tibble(`# variables` =  length(coef(.x))),
  .id = "method"
)
```

We can check the loss in each iteration, so you can choose what combintations
of loss/number of variables you want.

```{r}
do.call(plot, attr(fs, "variable_importance")) +
  ggplot2::scale_y_continuous(
    breaks = scales::pretty_breaks(7),
    sec.axis = ggplot2::dup_axis(~ 1 - .x, name = "AUC", labels = scales::percent)
  )

fs
```



